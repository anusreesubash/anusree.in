import { Content, BlogTitle, BlogSubtitle } from '../../../styles/blogStyles';

  <BlogTitle main> 
    reduce() and reduceByKey() in spark 
  </BlogTitle>
  reduce() and reduceByKey() are actions in spark that can be used to aggregate elements of a dataset(RDD) using a function.

  Lets first understand reduce() :

  reduce() accepts an iterator function as its input argument. This iterator function is called consecutively n-1 times (where n is the length of your dataset). During the first iteration the first 2 elements in the dataset are passed. The return value of the first iteration is used as the first argument for the second iteration. Below is an example that demonstrates reduce() in pyspark.


  Now, lets understand reduceByKey() :

  Here the dataset will be key-value pairs. They are grouped according to the keys before performing reduce() on each group. Below example illustrates reduceByKey() in pyspark.


